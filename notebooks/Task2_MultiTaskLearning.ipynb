{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Multi-Task Learning Expansion**\n",
        "\n",
        "**Background:**  \n",
        "In Task 2, we extend our SentenceTransformer (from Task 1) into a multi-task framework by:\n",
        "- **Task A:** Classifying sentences into predefined classes (e.g., three classes).\n",
        "- **Task B:** Performing sentiment analysis (e.g., binary classification: positive/negative).\n",
        "\n",
        "**Design Overview:**  \n",
        "- **Shared Encoder:** Use the SentenceTransformer to extract fixed-length embeddings.\n",
        "- **Task-Specific Heads:**  \n",
        "  - A two-layer feedforward network for sentence classification.\n",
        "  - A two-layer feedforward network for sentiment analysis.\n",
        "- This design enables shared representations with fine-tuning for each task.\n"
      ],
      "metadata": {
        "id": "61AR5uaLoZSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports and Setup\n",
        "\n",
        "# Import required libraries.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.sentence_transformer import SentenceTransformer\n",
        "\n",
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryNQBiBtkk-V",
        "outputId": "939cdfc4-7af4-48ad-f10c-804d54ac24b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78be91069ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation:**  \n",
        "- This block imports all necessary libraries, including PyTorch and the previously defined `SentenceTransformer` from Task 1. A fixed random seed is set to ensure that results are reproducible.\n",
        "\n"
      ],
      "metadata": {
        "id": "FrfMMvCgoxT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: MultiTaskModel Class Definition\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-task model that expands the SentenceTransformer for two tasks:\n",
        "      - Task A: Sentence Classification.\n",
        "      - Task B: Sentiment Analysis.\n",
        "\n",
        "    This model uses a shared encoder (the SentenceTransformer) and\n",
        "    two separate task-specific heads.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_model_name='bert-base-uncased',\n",
        "        num_classes_taskA=3,\n",
        "        num_classes_taskB=2\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the MultiTaskModel.\n",
        "\n",
        "        Args:\n",
        "            pretrained_model_name (str): Name of the pre-trained BERT model.\n",
        "            num_classes_taskA (int): Number of classes\n",
        "            for sentence classification.\n",
        "            num_classes_taskB (int): Number of classes for sentiment analysis.\n",
        "        \"\"\"\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        # Shared encoder: reusing the SentenceTransformer from Task 1.\n",
        "        self.encoder = SentenceTransformer(pretrained_model_name)\n",
        "        hidden_size = self.encoder.transformer.config.hidden_size\n",
        "\n",
        "        # Task A: Sentence Classification Head.\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_classes_taskA)\n",
        "        )\n",
        "\n",
        "        # Task B: Sentiment Analysis Head.\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_classes_taskB)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_sentences):\n",
        "        \"\"\"\n",
        "        Forward pass for the multi-task model.\n",
        "\n",
        "        Args:\n",
        "            input_sentences (list): A list of input sentence strings.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (logits_taskA, logits_taskB) where:\n",
        "                - logits_taskA: Output from the sentence classification head.\n",
        "                - logits_taskB: Output from the sentiment analysis head.\n",
        "        \"\"\"\n",
        "        # Generate sentence embeddings using the shared encoder.\n",
        "        embeddings = self.encoder(input_sentences)\n",
        "\n",
        "        # Process the embeddings through each task-specific head.\n",
        "        logits_taskA = self.classification_head(embeddings)\n",
        "        logits_taskB = self.sentiment_head(embeddings)\n",
        "\n",
        "        return logits_taskA, logits_taskB\n"
      ],
      "metadata": {
        "id": "bPn-uvkcpLCS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation:**\n",
        "\n",
        "- **Class Definition:**  \n",
        "  This step defines the `MultiTaskModel` class, which extends the SentenceTransformer from Task 1 by adding two separate task-specific heads:\n",
        "  - **Task A (Sentence Classification):** A two-layer feedforward network for classifying sentences.\n",
        "  - **Task B (Sentiment Analysis):** A two-layer feedforward network for analyzing sentiment.\n",
        "  \n",
        "- **Forward Method:**  \n",
        "  The `forward` method processes the input sentences by generating embeddings with the shared encoder and then passing those embeddings through each of the task-specific heads. The outputs (logits) for each task are returned as a tuple.\n",
        "\n"
      ],
      "metadata": {
        "id": "cK_wuy6_pW22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Testing the MultiTaskModel\n",
        "\n",
        "# Set up the computation device (GPU if available, else CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the MultiTaskModel (using 3 classes for Task A and 2 for Task B)\n",
        "# and move it to the device.\n",
        "multi_task_model = MultiTaskModel(\n",
        "    num_classes_taskA=3,\n",
        "    num_classes_taskB=2\n",
        "    ).to(device)\n",
        "\n",
        "# Define sample sentences for testing.\n",
        "sample_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Transformers have revolutionized natural language processing.\"\n",
        "]\n",
        "\n",
        "# Generate the shared sentence embeddings using the encoder.\n",
        "embeddings = multi_task_model.encoder(sample_sentences)\n",
        "\n",
        "# Pass the sample sentences through the multi-task model to obtain\n",
        "# logits for both tasks.\n",
        "logits_taskA, logits_taskB = multi_task_model(sample_sentences)\n",
        "\n",
        "# Print the shared embeddings and their shape.\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "print(\"Embeddings:\")\n",
        "print(embeddings.detach().cpu().numpy())\n",
        "\n",
        "# Print the output logits shapes for each task.\n",
        "print(\"Task A (Sentence Classification) Logits Shape:\", logits_taskA.shape)\n",
        "print(\"Task B (Sentiment Analysis) Logits Shape:\", logits_taskB.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moSf6Umop4o9",
        "outputId": "0a060f45-14b9-4a50-e406-5e72d2aeae0b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([2, 768])\n",
            "Embeddings:\n",
            "[[-0.36080578  0.22707793 -0.3029696  ... -0.42242897  0.69488996\n",
            "   0.62128514]\n",
            " [-0.44105065 -0.14493446 -0.28509557 ... -0.32411036 -0.15410942\n",
            "   0.34400165]]\n",
            "Task A (Sentence Classification) Logits Shape: torch.Size([2, 3])\n",
            "Task B (Sentiment Analysis) Logits Shape: torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Explanation:**  \n",
        "In this testing block:\n",
        "- The computation device is set to GPU if available; otherwise, CPU is used.\n",
        "- The `MultiTaskModel` is instantiated with 3 classes for Task A and 2 classes for Task B.\n",
        "- Sample sentences are processed to generate:\n",
        "  - **Shared Embeddings:** Expected shape `(2, hidden_size)` (e.g., `(2, 768)` for BERT).\n",
        "  - **Task A Logits:** Expected shape `(2, 3)` for sentence classification.\n",
        "  - **Task B Logits:** Expected shape `(2, 2)` for sentiment analysis.\n",
        "- This block demonstrates that the model correctly processes input sentences and outputs raw logits for each task, which is the primary objective of Task 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uz4sr1W0r7CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architectural Changes for Multi-Task Learning (Task 2)**\n",
        "\n",
        "- **Shared Encoder:**  \n",
        "  We reuse the pre-trained SentenceTransformer from Task 1 to extract fixed-length embeddings for all tasks. This ensures consistent representations and reduces the number of parameters.\n",
        "\n",
        "- **Task-Specific Heads:**  \n",
        "  Two separate feedforward networks are added on top of the shared encoder:  \n",
        "  - **Task A (Sentence Classification):** A two-layer network that outputs logits for a predefined number of classes (e.g., three classes).  \n",
        "  - **Task B (Sentiment Analysis):** A two-layer network that outputs logits for binary sentiment (e.g., positive/negative).\n",
        "\n",
        "- **Output as Logits:**  \n",
        "  The model returns raw logits for each task, allowing the use of loss functions (like CrossEntropyLoss) during training. Predictions can be derived by applying softmax and argmax during inference.\n",
        "\n",
        "- **Modular Design:**  \n",
        "  By separating the shared encoder and task-specific heads, the architecture is flexible—each head can be fine-tuned independently while sharing common features. This modularity also facilitates future expansion to additional tasks."
      ],
      "metadata": {
        "id": "q3nwOMhCxhFC"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Fetch ML Assessment: Task 1  \n",
        "## Sentence Transformer Implementation Using Pre-trained BERT\n",
        "\n",
        "\n",
        "> **Background:**  \n",
        "> In this assessment, our goal is to implement a sentence transformer model that encodes input sentences into fixed-length embeddings. We will use a pre-trained BERT model (specifically `bert-base-uncased`) and extract the [CLS] token's output as the sentence embedding. This implementation lays the foundation for later extensions into multi-task learning.  \n",
        ">  \n",
        "> **Design Priorities:**  \n",
        "> - **Clarity & Modularity:** The code is split into clearly defined steps, and each function/class is documented.  \n",
        "> - **Efficiency:** By leveraging a pre-trained model, we ensure that our sentence representations are robust and efficient.  \n",
        "> - **Reproducibility:** A fixed random seed is set to help reproduce results.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C8E_Sd3xR_t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Environment Setup\n",
        "\n",
        "# Install the transformers library if it is not already installed using pip.\n",
        "# Uncomment the following line if installation is required.\n",
        "# !pip install transformers\n",
        "\n",
        "# Import required libraries.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Set the random seed for reproducibility.\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkpuAN5XSgRj",
        "outputId": "fd619b6c-954e-4e80-853a-9ab3329a9fa4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x796ace016070>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Explanation\n",
        "\n",
        "> - In this step, we import all the necessary libraries:  \n",
        "> - **PyTorch**: Used to build our neural network model and handle tensor operations.  \n",
        "> - **Hugging Face Transformers**: Provides the pre-trained BERT model and its tokenizer.  \n",
        "> We also set a random seed (`torch.manual_seed(42)`) to ensure that our results remain consistent across runs.\n"
      ],
      "metadata": {
        "id": "cJQchIYKTEy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Implementing the Sentence Transformer Class\n",
        "\n",
        "\n",
        "# Define the SentenceTransformer class which uses a pre-trained BERT model.\n",
        "class SentenceTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    A sentence transformer model that converts input sentences into\n",
        "    fixed-length embeddings.\n",
        "    Utilizes a pre-trained BERT model and extracts the [CLS] token as the\n",
        "    sentence embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
        "        \"\"\"\n",
        "        Initializes the SentenceTransformer.\n",
        "\n",
        "        Args:\n",
        "            pretrained_model_name (str): Name of the pre-trained BERT\n",
        "            model to load.\n",
        "        \"\"\"\n",
        "        super(SentenceTransformer, self).__init__()\n",
        "        # Load the pre-trained BERT model and its corresponding tokenizer.\n",
        "        self.transformer = BertModel.from_pretrained(pretrained_model_name)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "\n",
        "    def forward(self, input_sentences):\n",
        "        \"\"\"\n",
        "        Forward pass for the model. Converts input sentences into embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_sentences (list): List of input sentence strings.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of shape (batch_size, hidden_size)\n",
        "            representing the sentence embeddings.\n",
        "        \"\"\"\n",
        "        # Tokenize the input sentences with padding and truncation.\n",
        "        encoded_input = self.tokenizer(\n",
        "            input_sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move the token tensors to the same device as the model parameters.\n",
        "        input_ids = encoded_input['input_ids'].to(\n",
        "            next(self.parameters()).device)\n",
        "        attention_mask = encoded_input['attention_mask'].to(\n",
        "            next(self.parameters()).device)\n",
        "\n",
        "        # Obtain outputs from the BERT transformer.\n",
        "        outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the embedding of the [CLS] token (the first token)\n",
        "        # as the sentence representation.\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        return cls_embeddings"
      ],
      "metadata": {
        "id": "TiceE4F6TfeH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code Explanation**\n",
        "\n",
        "> - This cell defines the `SentenceTransformer` class.  \n",
        ">  \n",
        "> - **Key Points:**  \n",
        "> - **Initialization:** The constructor loads the pre-trained BERT model (`bert-base-uncased`) along with its tokenizer.  \n",
        "> - **Forward Method:**  \n",
        ">   - Tokenizes input sentences (using padding and truncation) to create input IDs and attention masks.  \n",
        ">   - Moves these tokenized inputs to the same device (CPU or GPU) as the model.  \n",
        ">   - Passes the tokens through the BERT model and extracts the [CLS] token's embedding as the fixed-length representation for each sentence.  \n"
      ],
      "metadata": {
        "id": "sXaGgBe4UU_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Testing the Sentence Transformer\n",
        "\n",
        "# Set up the device for computation: use GPU if available, otherwise CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of the SentenceTransformer and move it to the\n",
        "# selected device.\n",
        "model = SentenceTransformer().to(device)\n",
        "\n",
        "# Define a list of sample sentences to test the model.\n",
        "sample_sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Transformers are very effective for NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for the sample sentences.\n",
        "embeddings = model(sample_sentences)\n",
        "\n",
        "# Print the shape of the generated embeddings.\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "# Print the actual embedding values.\n",
        "print(\"Embeddings:\")\n",
        "print(embeddings.detach().cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUgM21fLUTzD",
        "outputId": "33053e86-a019-47b1-c10d-e33a480f4b3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([2, 768])\n",
            "Embeddings:\n",
            "[[-0.36080578  0.22707793 -0.3029696  ... -0.42242897  0.69488996\n",
            "   0.62128514]\n",
            " [-0.41661704 -0.15820043  0.14587061 ... -0.62564856 -0.02686349\n",
            "   0.44702652]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Code Explanation**  \n",
        "> In this testing step:  \n",
        "> - **Device Selection:** We set the computation device to GPU if available; otherwise, the CPU is used.  \n",
        "> - **Model Instantiation:** An instance of `SentenceTransformer` is created and moved to the selected device.  \n",
        "> - **Testing:** Two sample sentences are processed through the model to generate embeddings.  \n",
        "> - **Output Verification:** The printed output should display a tensor shape of `torch.Size([2, 768])`, assuming BERT's hidden size is 768.  \n",
        ">  \n",
        "> This confirms that our model correctly generates fixed-length sentence embeddings.\n",
        "\n",
        "\n",
        "**Architectural Decisions Outside the Transformer Backbone**\n",
        "\n",
        "Outside the transformer backbone, we made three key decisions:  \n",
        "1. We chose to use the [CLS] token for pooling due to its ability to capture sentence-level meaning.  \n",
        "2. We intentionally avoided adding extra layers to keep the model simple and mitigate overfitting risks.  \n",
        "3. We implemented device consistency by moving all input tensors to the same device as the model."
      ],
      "metadata": {
        "id": "kiKFjcaCVLST"
      }
    }
  ]
}